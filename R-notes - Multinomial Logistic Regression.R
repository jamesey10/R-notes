

# Multinomial Logistic Regression | R Data Analysis Examples --------------
# Multinomial logistic regression is used to
# model nominal outcome variables,
# in which the log odds of the outcomes are modeled as 
# a linear combination of the predictor variables.

require(foreign)
require(nnet)
require(ggplot2)
require(reshape2)

ml <- read.dta("https://stats.idre.ucla.edu/stat/data/hsbdemo.dta")
#The data set contains variables on 200 students. 
#The outcome variable is "prog", program type. 
# The predictor variables are 
# social economic status, "ses", a three-level categorical variable
# and writing score, "write", a continuous variable. 
head(ml)

with(ml, table(ses, prog))
with(ml, do.call(rbind, tapply(write, prog, function(x) c(M = mean(x), SD = sd(x)))))


# use the multinom function from the nnet package 
# to estimate a multinomial logistic regression model
#we choose the multinom function because it does not require 
#the data to be reshaped (as the mlogit package does)

#copy the prog column (program) to a new column called prog 2
ml$prog2 <- relevel(ml$prog, ref = "academic")
head(ml)
test <- multinom(prog2 ~ ses + write, data = ml)

# We first see that some output is generated by running the model
# even though we are assigning the model to a new R object.
# This model-running output includes some iteration history 
# and includes the final negative log-likelihood 179.981726. 
# This value multiplied by two is then seen in the model summary 
# as the Residual Deviance and it can be used in comparisons of nested models,

summary(test)
# Residual Difference is 360ish.... 359.9635 

z <- summary(test)$coefficients/summary(test)$standard.errors
z

# 2-tailed z test
p <- (1 - pnorm(abs(z), 0, 1)) * 2
p

# we want the logarthimics converted to coefficiencts
exp(coef(test))


# The relative risk ratio for a one-unit increase in the variable write is
# 0.9437 for being in general program vs. academic program.
# The relative risk ratio switching from ses = 1 to 3 is
# 0.3126 for being in general program vs. academic program.

# calculate predicted probabilities for each of our outcome levels
#using the fitted function.
head(pp <- fitted(test))

# examine the changes in predicted probability associated 
#with one of our two variables,
# by creating small datasets varying one variable 
# while holding the other constant.  
# first do this holding write at its mean
# and examining the predicted probabilities for each level of ses.

dses <- data.frame(ses = c("low", "middle", "high"), write = mean(ml$write))
predict(test, newdata = dses, "probs")

#look at the averaged predicted probabilities for different values 
# of the continuous predictor variable write within each level of ses.
dwrite <- data.frame(ses = rep(c("low", "middle", "high"),
                          each = 41), write = rep(c(30:70),
                                                        3))

## store the predicted probabilities for each value of ses and write
pp.write <- cbind(dwrite, predict(test, newdata = dwrite, type = "probs", se = TRUE))

## calculate the mean probabilities within each level of ses
by(pp.write[, 3:5], pp.write$ses, colMeans)


## melt data set to long for ggplot2
lpp <- melt(pp.write, id.vars = c("ses", "write"), value.name = "probability")
head(lpp)  # view first few rows


## plot predicted probabilities across write values for each level of ses
## facetted by program type
ggplot(lpp, aes(x = write, y = probability, colour = ses)) + geom_line() +
  facet_grid(variable ~  ., scales = "free")
